{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, glob\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "import open3d as o3d\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import smplx\n",
    "from human_body_prior.tools.model_loader import load_vposer\n",
    "\n",
    "from utils import GeometryTransformer, BodyParamParser\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import chamfer_pytorch.dist_chamfer as ext\n",
    "from torch.autograd import Variable\n",
    "import trimesh\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_params_encapsulate(x_body_rec, to_numpy=True, batched=False,nobody=True):\n",
    "    \n",
    "    if to_numpy:\n",
    "        x_body_rec_np = x_body_rec.detach().cpu().numpy()\n",
    "    else:\n",
    "        x_body_rec_np = x_body_rec\n",
    "        \n",
    "    \n",
    "    if batched:\n",
    "        if nobody:\n",
    "            body_params_batch_rec={}\n",
    "            body_params_batch_rec['transl'] = x_body_rec_np[:,:3]\n",
    "            body_params_batch_rec['global_orient'] = x_body_rec_np[:,3:6]\n",
    "            #body_params_batch_rec['betas'] = x_body_rec_np[:,6:16]\n",
    "            body_params_batch_rec['body_pose'] = x_body_rec_np[:,6:38]\n",
    "            body_params_batch_rec['left_hand_pose'] = x_body_rec_np[:,38:50]\n",
    "            body_params_batch_rec['right_hand_pose'] = x_body_rec_np[:,50:]\n",
    "        \n",
    "        else:\n",
    "            body_params_batch_rec={}\n",
    "            body_params_batch_rec['transl'] = x_body_rec_np[:,:3]\n",
    "            body_params_batch_rec['global_orient'] = x_body_rec_np[:,3:6]\n",
    "            body_params_batch_rec['betas'] = x_body_rec_np[:,6:16]\n",
    "            body_params_batch_rec['body_pose'] = x_body_rec_np[:,16:48]\n",
    "            body_params_batch_rec['left_hand_pose'] = x_body_rec_np[:,48:60]\n",
    "            body_params_batch_rec['right_hand_pose'] = x_body_rec_np[:,60:]\n",
    "\n",
    "        \n",
    "        return body_params_batch_rec\n",
    "    \n",
    "    else:\n",
    "        n_batch = x_body_rec_np.shape[0]\n",
    "        rec_list = []\n",
    "\n",
    "        for b in range(n_batch):\n",
    "            body_params_batch_rec={}\n",
    "            body_params_batch_rec['transl'] = x_body_rec_np[b:b+1,:3]\n",
    "            body_params_batch_rec['global_orient'] = x_body_rec_np[b:b+1,3:6]\n",
    "            body_params_batch_rec['betas'] = x_body_rec_np[b:b+1,6:16]\n",
    "            body_params_batch_rec['body_pose'] = x_body_rec_np[b:b+1,16:48]\n",
    "            #body_params_batch_rec['left_hand_pose'] = x_body_rec_np[b:b+1,48:60]\n",
    "            #body_params_batch_rec['right_hand_pose'] = x_body_rec_np[b:b+1,60:]\n",
    "            rec_list.append(body_params_batch_rec)\n",
    "\n",
    "        return rec_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Trained Model: ./vposer_v1_0\\snapshots\\TR00_E096.pt\n"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "vposer, _ = load_vposer('./vposer_v1_0', vp_model='snapshot')\n",
    "vposer=vposer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mesh_model_batch = smplx.create('./models', \n",
    "                               model_type='smplx',\n",
    "                               gender='neutral', ext='npz',\n",
    "                               num_pca_comps=12,\n",
    "                               create_global_orient=True,\n",
    "                               create_body_pose=True,\n",
    "                               create_betas=True,\n",
    "                               create_left_hand_pose=True,\n",
    "                               create_right_hand_pose=True,\n",
    "                               create_expression=True,\n",
    "                               create_jaw_pose=True,\n",
    "                               create_leye_pose=True,\n",
    "                               create_reye_pose=True,\n",
    "                               create_transl=True,\n",
    "                               batch_size=60\n",
    "                               )\n",
    "body_mesh_model_batch=body_mesh_model_batch.to(device)\n",
    "\n",
    "body_mesh_model = smplx.create('./models', \n",
    "                               model_type='smplx',\n",
    "                               gender='neutral', ext='npz',\n",
    "                               num_pca_comps=12,\n",
    "                               create_global_orient=True,\n",
    "                               create_body_pose=True,\n",
    "                               create_betas=True,\n",
    "                               create_left_hand_pose=True,\n",
    "                               create_right_hand_pose=True,\n",
    "                               create_expression=True,\n",
    "                               create_jaw_pose=True,\n",
    "                               create_leye_pose=True,\n",
    "                               create_reye_pose=True,\n",
    "                               create_transl=True,\n",
    "                               batch_size=1\n",
    "                               )\n",
    "body_mesh_model=body_mesh_model.to(device)\n",
    "\n",
    "body_mesh_model_input = smplx.create('./models', \n",
    "                               model_type='smplx',\n",
    "                               gender='neutral', ext='npz',\n",
    "                               num_pca_comps=12,\n",
    "                               create_global_orient=True,\n",
    "                               create_body_pose=True,\n",
    "                               create_betas=True,\n",
    "                               create_left_hand_pose=True,\n",
    "                               create_right_hand_pose=True,\n",
    "                               create_expression=True,\n",
    "                               create_jaw_pose=True,\n",
    "                               create_leye_pose=True,\n",
    "                               create_reye_pose=True,\n",
    "                               create_transl=True,\n",
    "                               batch_size=2\n",
    "                               )\n",
    "\n",
    "body_mesh_model_input=body_mesh_model_input.to(device)\n",
    "contact_id_folder='./data/body_segments'\n",
    "contact_part=['back','gluteus','L_Hand','R_Hand','L_Leg','R_Leg','thighs']\n",
    "foot_vid, foot_fid = GeometryTransformer.get_contact_id(body_segments_folder=contact_id_folder,\n",
    "                            contact_body_parts=['L_Leg','R_Leg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub goal body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sub_goal import SUBGOAL\n",
    "ndim=65\n",
    "cvae=SUBGOAL(n_dim_body=ndim,scene_model_ckpt=False,device=device)\n",
    "cvae=cvae.to(device)\n",
    "cvae.load_state_dict(torch.load('saved_model/subgoal.model',map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MPH16.ply is a downsampled scene from PROXE dataset\n",
    "scene_mesh=o3d.io.read_triangle_mesh('demo_data/MPH16.ply')\n",
    "\n",
    "#camera_ext of MPH16\n",
    "camera_ext=np.array([[-0.40142276,  0.34250608, -0.84944061,  2.16673488],\n",
    "                   [ 0.91529181,  0.11642218, -0.38559925,  1.88633201],\n",
    "                   [-0.03317636, -0.93227435, -0.36022753,  0.736902  ],\n",
    "                   [ 0.        ,  0.        ,  0.        ,  1.        ]])\n",
    "camera_ext_inv=np.linalg.inv(camera_ext)\n",
    "\n",
    "# apply transformation to the scene pointcloud, let the scene and human body in the same area\n",
    "scene_points=torch.tensor(np.asarray(scene_mesh.transform(camera_ext_inv).vertices),dtype=torch.float32).to(device)\n",
    "\n",
    "#scene_points=torch.tensor(scene_points,dtype=torch.float32).to(device)\n",
    "#you can randomly sample the positions or get them from the dataset\n",
    "\n",
    "start=GeometryTransformer.convert_to_6D_rot(\n",
    "    torch.tensor([[-0.6111, -0.0384,  2.8900, -2.8275, -0.0543, -0.2241]])).to(device)\n",
    "sub=GeometryTransformer.convert_to_6D_rot(\n",
    "    torch.tensor([[-0.3502,  0.1911,  2.1834, -2.8251,  0.0991, -0.7367]])).to(device)\n",
    "end=GeometryTransformer.convert_to_6D_rot(\n",
    "    torch.tensor([[-0.6594,  0.2947,  1.2586, -2.8201, -0.1839,  0.5844]])).to(device)\n",
    "\n",
    "#you can set the body shape parameters\n",
    "body=torch.zeros([1,10]).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "start_body = cvae.sample(None,scene_points.unsqueeze(0).transpose(1,2),start,body)\n",
    "start_body_ = GeometryTransformer.convert_to_3D_rot(start_body)\n",
    "body_param_rec = BodyParamParser.body_params_encapsulate_batch_nobody_hand(start_body_)\n",
    "body_param_rec['body_pose'] = vposer.decode(body_param_rec['body_pose'], \n",
    "                                output_type='aa').view(start_body.shape[0], -1)\n",
    "body_param_rec['betas']=body.repeat(start_body.shape[0],1)\n",
    "smplx_output = body_mesh_model(return_verts=True, \n",
    "                                      **body_param_rec\n",
    "                                     )\n",
    "body_verts_batch = smplx_output.vertices\n",
    "smplx_faces = body_mesh_model.faces\n",
    "out_mesh =trimesh.Trimesh(body_verts_batch[0].detach().cpu().numpy(),smplx_faces,process=False)\n",
    "out_mesh.apply_transform(camera_ext)\n",
    "out_mesh.export('./demo_data/start.obj')\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "middle_body = cvae.sample(None,scene_points.unsqueeze(0).transpose(1,2),sub,body)\n",
    "middle_body_ = GeometryTransformer.convert_to_3D_rot(middle_body)\n",
    "\n",
    "body_param_rec = BodyParamParser.body_params_encapsulate_batch_nobody_hand(middle_body_)\n",
    "body_param_rec['body_pose'] = vposer.decode(body_param_rec['body_pose'], \n",
    "                                output_type='aa').view(start_body.shape[0], -1)\n",
    "body_param_rec['betas']=body.repeat(start_body.shape[0],1)\n",
    "smplx_output = body_mesh_model(return_verts=True, \n",
    "                                      **body_param_rec\n",
    "                                     )\n",
    "body_verts_batch = smplx_output.vertices\n",
    "smplx_faces = body_mesh_model.faces\n",
    "out_mesh =trimesh.Trimesh(body_verts_batch[0].detach().cpu().numpy(),smplx_faces,process=False)\n",
    "out_mesh.apply_transform(camera_ext)\n",
    "out_mesh.export('./demo_data/middle.obj')\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "end_body = cvae.sample(None,scene_points.unsqueeze(0).transpose(1,2),end,body)\n",
    "end_body_ = GeometryTransformer.convert_to_3D_rot(end_body)\n",
    "\n",
    "body_param_rec = BodyParamParser.body_params_encapsulate_batch_nobody_hand(end_body_)\n",
    "body_param_rec['body_pose'] = vposer.decode(body_param_rec['body_pose'], \n",
    "                                output_type='aa').view(start_body.shape[0], -1)\n",
    "body_param_rec['betas']=body.repeat(start_body.shape[0],1)\n",
    "smplx_output = body_mesh_model(return_verts=True, \n",
    "                                      **body_param_rec\n",
    "                                     )\n",
    "body_verts_batch = smplx_output.vertices\n",
    "smplx_faces = body_mesh_model.faces\n",
    "out_mesh=trimesh.Trimesh(body_verts_batch[0].detach().cpu().numpy(),smplx_faces,process=False)\n",
    "out_mesh.apply_transform(camera_ext)\n",
    "out_mesh.export('./demo_data/end.obj')\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiash\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from route import ROUTENET\n",
    "from pose_after_route import POSEAFTERROUTE\n",
    "\n",
    "routenet = ROUTENET(input_dim=9,hid_dim=64,scene_model_ckpt=False,device=device).to(device)\n",
    "posenet = POSEAFTERROUTE(input_dim=65-9,hid_dim=256,scene_model_ckpt=False,device=device).to(device)\n",
    "\n",
    "routenet.load_state_dict(torch.load('saved_model/route.model',map_location=device))\n",
    "posenet.load_state_dict(torch.load('saved_model/pose_after_route.model',map_location=device))\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=torch.cat([start_body,middle_body]).unsqueeze(0)\n",
    "route=routenet(x1[:,:,:9],scene_points.unsqueeze(0).transpose(1,2))\n",
    "pose=posenet(x1[:,:,9:],scene_points.unsqueeze(0).transpose(1,2),route.view(route.shape[0],-1))\n",
    "\n",
    "y1=torch.cat([route,pose],dim=2).view(-1,65)\n",
    "y1_ = GeometryTransformer.convert_to_3D_rot(y1)\n",
    "\n",
    "body_param_rec = BodyParamParser.body_params_encapsulate_batch_nobody_hand(y1_)\n",
    "body_param_rec['body_pose'] = vposer.decode(body_param_rec['body_pose'], \n",
    "                                output_type='aa').view(y1.shape[0], -1)\n",
    "body_param_rec['betas']=body.repeat(y1.shape[0],1)\n",
    "smplx_output = body_mesh_model_batch(return_verts=True, \n",
    "                                      **body_param_rec\n",
    "                                     )\n",
    "body_verts_batch = smplx_output.vertices\n",
    "for i in range(60):\n",
    "    out_mesh =trimesh.Trimesh(body_verts_batch[i].detach().cpu().numpy(),smplx_faces,process=False)\n",
    "    out_mesh.apply_transform(camera_ext)\n",
    "    out_mesh.export('./demo_data/'+str(i)+'.obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=torch.cat([middle_body,end_body]).unsqueeze(0)\n",
    "route=routenet(x2[:,:,:9],scene_points.unsqueeze(0).transpose(1,2))\n",
    "pose=posenet(x2[:,:,9:],scene_points.unsqueeze(0).transpose(1,2),route.view(route.shape[0],-1))\n",
    "\n",
    "y2=torch.cat([route,pose],dim=2).view(-1,65)\n",
    "y2_ = GeometryTransformer.convert_to_3D_rot(y2)\n",
    "\n",
    "body_param_rec = BodyParamParser.body_params_encapsulate_batch_nobody_hand(y2_)\n",
    "body_param_rec['body_pose'] = vposer.decode(body_param_rec['body_pose'], \n",
    "                                output_type='aa').view(y2.shape[0], -1)\n",
    "body_param_rec['betas']=body.repeat(y2.shape[0],1)\n",
    "smplx_output = body_mesh_model_batch(return_verts=True, \n",
    "                                      **body_param_rec\n",
    "                                     )\n",
    "body_verts_batch = smplx_output.vertices\n",
    "for i in range(60):\n",
    "    out_mesh =trimesh.Trimesh(body_verts_batch[i].detach().cpu().numpy(),smplx_faces,process=False)\n",
    "    out_mesh.apply_transform(camera_ext)\n",
    "    out_mesh.export('./demo_data/'+str(i+60)+'.obj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./demo_data/MPH16.json') as f:\n",
    "        sdf_data = json.load(f)\n",
    "        grid_min = np.array(sdf_data['min'])\n",
    "        grid_max = np.array(sdf_data['max'])\n",
    "        grid_dim = sdf_data['dim']\n",
    "sdf = np.load('./demo_data/MPH16_sdf.npy').reshape(grid_dim, grid_dim, grid_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_grid_min = torch.tensor(grid_min, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "s_grid_max = torch.tensor(grid_max, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "s_sdf = torch.tensor(sdf, dtype=torch.float32).unsqueeze(0).to(device) \n",
    "cam_ext=torch.tensor(camera_ext,dtype=torch.float32).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "['right', 'right', 'right', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'right', 'right', 'right', 'right', 'right', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'right', 'right', 'right', 'right', 'left', 'left', 'left', 'left', 'left', 'left', 'right', 'right', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'left', 'left', 'left', 'right', 'right', 'right', 'right']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiash\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "['left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left', 'left']\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([120, 62])\n"
     ]
    }
   ],
   "source": [
    "from utils_optimize import fitting\n",
    "\n",
    "\n",
    "contact_id_folder='./data/body_segments'\n",
    "\n",
    "#connect two short-term motion\n",
    "x_whole=torch.cat([y1_.detach(),y2_.detach()]) #_,62\n",
    "start_end=torch.cat([start_body_.detach(),end_body_.detach()]) #_,62\n",
    "\n",
    "\n",
    "body_mesh_model_batch = smplx.create('./models', \n",
    "                               model_type='smplx',\n",
    "                               gender='neutral', ext='npz',\n",
    "                               num_pca_comps=12,\n",
    "                               create_global_orient=True,\n",
    "                               create_body_pose=True,\n",
    "                               create_betas=True,\n",
    "                               create_left_hand_pose=True,\n",
    "                               create_right_hand_pose=True,\n",
    "                               create_expression=True,\n",
    "                               create_jaw_pose=True,\n",
    "                               create_leye_pose=True,\n",
    "                               create_reye_pose=True,\n",
    "                               create_transl=True,\n",
    "                               batch_size=x_whole.shape[0]\n",
    "                               )\n",
    "body_mesh_model_batch=body_mesh_model_batch.to(device)\n",
    "\n",
    "\n",
    "#first state of the optimization\n",
    "fittingconfig={'init_lr_h': 0.02,\n",
    "                'num_iter':20, \n",
    "                'contact_part':['back','butt','L_Hand','R_Hand','L_Leg','R_Leg','thighs'],\n",
    "            }\n",
    "lossconfig={\n",
    "        'weight_loss_rec': 0,\n",
    "        'weight_loss_vposer':0,\n",
    "        'weight_contact': 2,\n",
    "        'weight_collision' : 2,\n",
    "        'weight_motion':0.5,\n",
    "        'weight_skating':0\n",
    "    }\n",
    "\n",
    "\n",
    "y_after = fitting(x_whole, body, cam_ext, \n",
    "                    scene_points, s_sdf, s_grid_min, s_grid_max,\n",
    "                    fittingconfig, lossconfig,start_end,vposer, body_mesh_model_batch, threshold=1.6, contact_id_folder=contact_id_folder, device=device)\n",
    "\n",
    "fittingconfig={'init_lr_h': 0.02,\n",
    "                'num_iter':50, \n",
    "                'contact_part':['back','butt','L_Hand','R_Hand','L_Leg','R_Leg','thighs'],\n",
    "                }\n",
    "\n",
    "lossconfig={\n",
    "        'weight_loss_rec': 0,\n",
    "        'weight_loss_vposer':0,\n",
    "        'weight_contact': 1,\n",
    "        'weight_collision' : 1,\n",
    "        'weight_motion':0.5,\n",
    "        'weight_skating':1\n",
    "    }\n",
    "    \n",
    "y_after = fitting(y_after.detach(), body, cam_ext, \n",
    "                    scene_points, s_sdf, s_grid_min, s_grid_max,\n",
    "                    fittingconfig, lossconfig,start_end,vposer, body_mesh_model_batch, threshold=1.6, contact_id_folder=contact_id_folder, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_param_rec = BodyParamParser.body_params_encapsulate_batch_nobody_hand(y_after)\n",
    "body_param_rec['body_pose'] = vposer.decode(body_param_rec['body_pose'], \n",
    "                                output_type='aa').view(y_after.shape[0], -1)\n",
    "body_param_rec['betas']=body.repeat(y_after.shape[0],1)\n",
    "smplx_output = body_mesh_model_batch(return_verts=True, \n",
    "                                      **body_param_rec\n",
    "                                     )\n",
    "body_verts_batch = smplx_output.vertices\n",
    "for i in range(120):\n",
    "    out_mesh =trimesh.Trimesh(body_verts_batch[i].detach().cpu().numpy(),smplx_faces,process=False)\n",
    "    out_mesh.apply_transform(camera_ext)\n",
    "    out_mesh.export('./demo_data/after'+str(i)+'.obj')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
