{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, glob\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import smplx\n",
    "from human_body_prior.tools.model_loader import load_vposer\n",
    "\n",
    "from utils import ContinousRotReprDecoder, GeometryTransformer, BodyParamParser\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_global_T(x_batch, cam_intrisic, max_depth):\n",
    "    xt_batch = x_batch[:,:3]\n",
    "    xr_batch = x_batch[:,3:]\n",
    "\n",
    "    fx_batch = cam_intrisic[:,0,0]\n",
    "    fy_batch = cam_intrisic[:,1,1]\n",
    "    px_batch = cam_intrisic[:,0,2]\n",
    "    py_batch = cam_intrisic[:,1,2]\n",
    "    s_ = 1.0 / torch.max(px_batch, py_batch)\n",
    "\n",
    "    z = (xt_batch[:, 2]+1.0)/2.0 * max_depth\n",
    "\n",
    "    x = xt_batch[:,0] * z / s_ / fx_batch\n",
    "    y = xt_batch[:,1] * z / s_ / fy_batch\n",
    "    \n",
    "    xt_batch_recoverd = torch.stack([x,y,z],dim=-1)\n",
    "\n",
    "    return torch.cat([xt_batch_recoverd, xr_batch],dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_3D_rot(x_batch):\n",
    "    xt = x_batch[:,:3]\n",
    "    xr = x_batch[:,3:9]\n",
    "    xb = x_batch[:,9:]\n",
    "\n",
    "    xr_mat = ContinousRotReprDecoder.decode(xr) # return [:,3,3]\n",
    "    xr_aa = ContinousRotReprDecoder.matrot2aa(xr_mat) # return [:,3]\n",
    "\n",
    "    return torch.cat([xt, xr_aa, xb], dim=-1)\n",
    "\n",
    "\n",
    "def body_params_encapsulate(x_body_rec, to_numpy=True, batched=False):\n",
    "    \n",
    "    if to_numpy:\n",
    "        x_body_rec_np = x_body_rec.detach().cpu().numpy()\n",
    "    else:\n",
    "        x_body_rec_np = x_body_rec\n",
    "        \n",
    "    \n",
    "    if batched:\n",
    "        body_params_batch_rec={}\n",
    "        body_params_batch_rec['transl'] = x_body_rec_np[:,:3]\n",
    "        body_params_batch_rec['global_orient'] = x_body_rec_np[:,3:6]\n",
    "        body_params_batch_rec['betas'] = x_body_rec_np[:,6:16]\n",
    "        body_params_batch_rec['body_pose'] = x_body_rec_np[:,16:48]\n",
    "        #body_params_batch_rec['left_hand_pose'] = x_body_rec_np[:,48:60]\n",
    "        #body_params_batch_rec['right_hand_pose'] = x_body_rec_np[:,60:]\n",
    "        \n",
    "        return body_params_batch_rec\n",
    "    \n",
    "    else:\n",
    "        n_batch = x_body_rec_np.shape[0]\n",
    "        rec_list = []\n",
    "\n",
    "        for b in range(n_batch):\n",
    "            body_params_batch_rec={}\n",
    "            body_params_batch_rec['transl'] = x_body_rec_np[b:b+1,:3]\n",
    "            body_params_batch_rec['global_orient'] = x_body_rec_np[b:b+1,3:6]\n",
    "            body_params_batch_rec['betas'] = x_body_rec_np[b:b+1,6:16]\n",
    "            body_params_batch_rec['body_pose'] = x_body_rec_np[b:b+1,16:48]\n",
    "            #body_params_batch_rec['left_hand_pose'] = x_body_rec_np[b:b+1,48:60]\n",
    "            #body_params_batch_rec['right_hand_pose'] = x_body_rec_np[b:b+1,60:]\n",
    "            rec_list.append(body_params_batch_rec)\n",
    "\n",
    "        return rec_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_preprocessing(img, modality, target_domain_size=[128, 128]):\n",
    "\n",
    "    \"\"\"\n",
    "    input:\n",
    "        - img (depthmap or semantic map): [height, width].\n",
    "        - modality: 'depth' or 'seg'\n",
    "    output:\n",
    "        canvas: with shape of target_domain_size, where the input is in the\n",
    "                center tightly, with shape target_domain_size\n",
    "        factor: the resizing factor\n",
    "    \"\"\"\n",
    "\n",
    "    # prepare the canvas\n",
    "    img_shape_o = img.shape\n",
    "    canvas = torch.zeros([1,1]+target_domain_size, dtype=torch.float32,\n",
    "                         device=torch.device(device))\n",
    "\n",
    "\n",
    "    # filter out unavailable values\n",
    "    if modality == 'depth':\n",
    "        img[img>6.0]=6.0\n",
    "\n",
    "    if modality == 'seg':\n",
    "        img[img>41] = 41\n",
    "\n",
    "\n",
    "\n",
    "    ## rescale to [-1,1]\n",
    "    max_val = torch.max(img)\n",
    "    _img = 2* img / max_val - 1.0\n",
    "\n",
    "    ## put _img to the canvas\n",
    "    if img_shape_o[0]>= img_shape_o[1]:\n",
    "        factor = float(target_domain_size[0]) / img_shape_o[0]\n",
    "        target_height = target_domain_size[0]\n",
    "        target_width = int(img_shape_o[1] * factor) //2 *2 \n",
    "\n",
    "        # for depth map we use bilinear interpolation in resizing\n",
    "        # for segmentation map we use bilinear interpolation as well.\n",
    "        # note that float semantic label is not real in practice, but\n",
    "        # helpful in our work\n",
    "        target_size = [target_height, target_width]\n",
    "\n",
    "        _img = _img.view(1,1,img_shape_o[0],img_shape_o[1])\n",
    "        img_resize = F.interpolate(_img, size=target_size, mode='bilinear',\n",
    "                                    align_corners=False)\n",
    "\n",
    "        na = target_width\n",
    "        nb = target_domain_size[1]\n",
    "        lower = (nb //2) - (na //2)\n",
    "        upper = (nb //2) + (na //2)\n",
    "\n",
    "        canvas[:,:,:, lower:upper] = img_resize\n",
    "\n",
    "\n",
    "    else:\n",
    "        factor = float(target_domain_size[1]) / img_shape_o[1]\n",
    "\n",
    "        target_height = int(factor*img_shape_o[0]) //2 *2\n",
    "        target_width = target_domain_size[1]\n",
    "\n",
    "        target_size = [target_height, target_width]\n",
    "        _img = _img.view(1,1,img_shape_o[0],img_shape_o[1])\n",
    "        img_resize = F.interpolate(_img, size=target_size, mode='bilinear',\n",
    "                                    align_corners=False)\n",
    "\n",
    "        na = target_height\n",
    "        nb = target_domain_size[0]\n",
    "        lower = (nb //2) - (na //2)\n",
    "        upper = (nb //2) + (na //2)\n",
    "\n",
    "        canvas[:,:,lower:upper, :] = img_resize\n",
    "\n",
    "    return canvas, factor, max_val\n",
    "\n",
    "\n",
    "device=\"cpu\"\n",
    "\n",
    "def scipy_matfile_parse(filename):\n",
    "    '''\n",
    "    parse data from files and put them to GPU\n",
    "    Note that this function is for demo, and is different from the ones used in other places.\n",
    "    '''\n",
    "    data = sio.loadmat(filename)\n",
    "    depth0_np = data['depth']\n",
    "    seg0_np = data['seg']\n",
    "\n",
    "    ## change them to torch tensor\n",
    "    depth0 = torch.tensor(depth0_np, dtype=torch.float32, device=torch.device(device))\n",
    "    seg0 = torch.tensor(seg0_np, dtype=torch.float32, device=torch.device(device))\n",
    "\n",
    "    ## pre_processing\n",
    "    depth, factor_d,max_d = data_preprocessing(depth0, 'depth', target_domain_size=[128, 128])\n",
    "    seg, factor_s,_ = data_preprocessing(seg0, 'seg', target_domain_size=[128, 128])\n",
    "\n",
    "\n",
    "    cam_intrinsic_np = data['cam'][0][0]['intrinsic']\n",
    "    cam_intrinsic = torch.tensor(cam_intrinsic_np, dtype=torch.float32, device=torch.device(device)).unsqueeze(0)\n",
    "    cam_extrinsic_np = data['cam'][0][0]['extrinsic']\n",
    "    cam_extrinsic_np = np.linalg.inv(cam_extrinsic_np)\n",
    "    cam_extrinsic = torch.tensor(cam_extrinsic_np, dtype=torch.float32, device=torch.device(device)).unsqueeze(0)\n",
    "\n",
    "    return depth, seg, max_d.view(1), cam_intrinsic, cam_extrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data):\n",
    "    dataset=[]\n",
    "    for i in range(len(data)):\n",
    "        body_t = data[i]['transl']\n",
    "        body_r = data[i]['global_orient']\n",
    "        body_shape=data[i]['betas']\n",
    "        body_pose = data[i]['pose_embedding']\n",
    "        body_lhp = data[i]['left_hand_pose']\n",
    "        body_rhp = data[i]['right_hand_pose']\n",
    "        body = np.concatenate([body_t, body_r, body_shape, \n",
    "                               body_pose, body_lhp, body_rhp\n",
    "                               ],\n",
    "                               axis=-1)\n",
    "       \n",
    "        dataset.append(body)\n",
    "    #train_input=[]\n",
    "    #train_gt=[]\n",
    "    #test_input=[]\n",
    "    #test_gt=[]\n",
    "    #np.random.seed(2020)\n",
    "    #sample=np.random.choice(len(data),int(len(data)*0.8),replace=False)\n",
    "    input_list=[]\n",
    "    gt_list=[]\n",
    "   \n",
    "    for i in range(len(dataset)):\n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            exist1=0\n",
    "            exist2=0\n",
    "            input1_list=[]\n",
    "            for j in range(i,i-40,-1):\n",
    "                if 0.2<np.mean((dataset[i][0][:3]-dataset[j][0][:3])**2)<10:\n",
    "                    \n",
    "                    input1_list.append(dataset[j])\n",
    "                    input1_=dataset[j]\n",
    "                    exist1=1\n",
    "            \n",
    "            #print('input1:',len(input1))\n",
    "            #print('input1_:',len(input1_))\n",
    "            input2_list=[]\n",
    "            for k in range(i,i+40,1):\n",
    "                if 0.2<np.mean((dataset[i][0][:3]-dataset[k][0][:3])**2)<10:\n",
    "                    input2_list.append(dataset[k])\n",
    "                    input2_=dataset[k]\n",
    "                    exist2=1\n",
    "            \n",
    "            #print('input2:',len(input2))\n",
    "            #print('input2_:',len(input2_))\n",
    "            if exist1==1 and exist2==1:\n",
    "                input1=random.choice(input1_list)\n",
    "                input2=random.choice(input2_list)\n",
    "                \n",
    "            \n",
    "                input_=torch.tensor(np.concatenate([input1,input2]), dtype=torch.float32)\n",
    "\n",
    "                middle=torch.tensor(dataset[i]                                             \n",
    "                               ,dtype=torch.float32)\n",
    "\n",
    "            #if i in sample:\n",
    "            #    train_input.append(input)\n",
    "            #    train_gt.append(gt)\n",
    "            #else:\n",
    "            #    test_input.append(input)\n",
    "            #    test_gt.append(gt)\n",
    "            #if torch.max(np.isnan(input_))==0 and torch.max(np.isnan(middle))==0:\n",
    "            if torch.max(np.isnan(input_))==0 and torch.max(np.isnan(middle))==0:\n",
    "                input_list.append(input_)\n",
    "                gt_list.append(middle)\n",
    "            \n",
    "            else:\n",
    "                print(1)\n",
    "        except:\n",
    "            pass\n",
    "       \n",
    "    #print(len(input_list))\n",
    "    #return train_input,train_gt,test_input,test_gt\n",
    "    #return input_list,gt_list\n",
    "    \n",
    "    return input_list,gt_list,dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train BasementSittingBooth_00142_01 66\n",
      "train BasementSittingBooth_00145_01 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiash\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in greater\n",
      "C:\\Users\\jiash\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train BasementSittingBooth_03452_01 113\n",
      "train MPH112_00034_01 83\n",
      "train MPH112_00150_01 77\n",
      "train MPH112_00151_01 46\n",
      "train MPH112_00157_01 43\n",
      "train MPH112_00169_01 78\n",
      "train MPH112_03515_01 60\n",
      "train MPH11_00034_01 105\n",
      "train MPH11_00150_01 106\n",
      "train MPH11_00151_01 58\n",
      "train MPH11_03515_01 88\n",
      "test MPH16_00157_01 79\n",
      "test MPH16_03301_01 62\n",
      "test MPH1Library_00034_01 108\n",
      "train MPH8_00168_01 162\n",
      "train MPH8_03301_01 96\n",
      "test N0SittingBooth_00162_01 59\n",
      "test N0SittingBooth_00169_01 34\n",
      "test N0SittingBooth_00169_02 43\n",
      "test N0SittingBooth_03301_01 33\n",
      "test N0SittingBooth_03403_01 51\n",
      "train N0Sofa_00034_01 163\n",
      "train N0Sofa_00034_02 57\n",
      "train N0Sofa_00141_01 112\n",
      "train N0Sofa_00145_01 101\n",
      "train N3Library_00157_01 25\n",
      "train N3Library_00157_02 8\n",
      "train N3Library_03301_01 16\n",
      "train N3Library_03301_02 4\n",
      "train N3Library_03375_01 34\n",
      "train N3Library_03375_02 450\n",
      "train N3Library_03403_01 5\n",
      "train N3Library_03403_02 26\n",
      "train N3Office_00034_01 98\n",
      "train N3Office_00139_01 50\n",
      "train N3Office_00139_02 109\n",
      "train N3Office_00150_01 126\n",
      "train N3Office_00153_01 60\n",
      "train N3Office_00159_01 96\n",
      "train N3Office_03301_01 97\n",
      "test N3OpenArea_00157_01 27\n",
      "test N3OpenArea_00157_02 49\n",
      "test N3OpenArea_00158_01 48\n",
      "test N3OpenArea_00158_02 88\n",
      "test N3OpenArea_03301_01 31\n",
      "test N3OpenArea_03403_01 19\n",
      "train Werkraum_03301_01 21\n",
      "train Werkraum_03403_01 21\n",
      "train Werkraum_03516_01 93\n",
      "train Werkraum_03516_02 63\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "sdf_path = scene_sdf_path='./data/prox/sdf' \n",
    "        \n",
    "sub_data_testing=[]\n",
    "sub_data_training=[]\n",
    "total_file_list=sorted(os.listdir('./data/prox/PROXD/'))\n",
    "testing_scene= ['MPH16','MPH1Library', 'N0SittingBooth','N3OpenArea']\n",
    "for each in total_file_list:\n",
    "    scene_name=each[:-9]\n",
    "    with open(os.path.join(scene_sdf_path, scene_name+'.json')) as f:\n",
    "        sdf_data = json.load(f)\n",
    "        grid_min = np.array(sdf_data['min'])\n",
    "        grid_max = np.array(sdf_data['max'])\n",
    "        grid_dim = sdf_data['dim']\n",
    "    sdf = np.load(os.path.join(scene_sdf_path, scene_name + '_sdf.npy')).reshape(grid_dim, grid_dim, grid_dim)\n",
    "    scene_name = each[:-9]\n",
    "    filelist = os.listdir('./data/prox/PROXD/'+each+'/results')\n",
    "    filelist = sorted(filelist)\n",
    "    temp_data_file = filelist\n",
    "    data = []\n",
    "    for i in range(len(temp_data_file)):\n",
    "        f = open('./data/prox/PROXD/'+each+'/results/'+temp_data_file[i]+'/000.pkl','rb')\n",
    "        temp = pickle.load(f)\n",
    "        data.append(temp)\n",
    "    sample_data = []\n",
    "    for i in range(0,len(data),15):\n",
    "        sample_data.append(data[i])\n",
    "    input_list,middle_list,ds = create_dataset(sample_data)\n",
    "    \n",
    "\n",
    "    \n",
    "    scene_mesh = o3d.io.read_triangle_mesh('./data/Proxe/scenes_downsampled/'+scene_name+'.ply')\n",
    "    \n",
    "    \n",
    "    cam_ext_path = './data/prox/cam2world/'+scene_name+'.json'\n",
    "    f = open(cam_ext_path,'r')\n",
    "    contents = f.read();\n",
    "    cam_ext = json.loads(contents)\n",
    "    cam_ = np.linalg.inv(cam_ext)\n",
    "    \n",
    "    \n",
    "    scene_verts = torch.tensor(np.asarray(scene_mesh.transform(cam_).vertices),dtype=torch.float32)\n",
    "    \n",
    "    grid_min = torch.tensor(np.array(sdf_data['min']),dtype=torch.float32)\n",
    "    grid_max = torch.tensor(np.array(sdf_data['max']),dtype=torch.float32)\n",
    "    if scene_name in testing_scene:\n",
    "        for i in range(len(middle_list)):\n",
    "            sub_data_testing.append([middle_list[i],each,scene_name,sdf,scene_verts,np.array(cam_ext),grid_min,grid_max])\n",
    "        print('test',each,i)\n",
    "    else:\n",
    "        for i in range(len(middle_list)):\n",
    "            sub_data_training.append([middle_list[i],each,scene_name,sdf,scene_verts,np.array(cam_ext),grid_min,grid_max])\n",
    "        print('train',each,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/sub_data_training.npy',np.array(sub_data_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/sub_data_testing.npy',np.array(sub_data_training))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
