
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title> Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes </title>
    
    <link href="style.css" rel="stylesheet">

    <style>
      body {
        font-family: Arial;
        font-size:15px;
        margin: 60px auto;
        width: auto;
        max-width: 1200px;
      }



      hr {
        border: 0;
        height: 1.0px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3));
      }

      .gap-30 {
      width:100%;
      height:30px;
      }

      .gap-20 {
      width:100%;
      height:20px;
      }

      .gap-10 {
      width:100%;
      height:10px;
      }

      .gap-5 {
      width:100%;
      height:5px;
      }

      .no-gutters {
      margin-right: 0;
      margin-left: 0;

      > .col,
      > [class*="col-"] {
        padding-right: 0;
        padding-left: 0;
      }
    }

    </style>

  </head>

  <div class="container">

    <center><span style="font-size:36px"> Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes </span></center>
    <div class="gap-5"></div>

    <!--------------------- Author Names --------------------->
    <div class="row">
      <div class="col-md-1">
      </div>
      <div class="col-md-2">
        <center><span style="font-size:16px">
          <a href="https://github.com/jiashunwang">Jiashun Wang<sup style="font-size:10px">1</sup></a>
        </span></center>
      </div>

      <div class="col-md-2">
        <center><span style="font-size:16px">
          <a href="http://hxu.rocks/">Huazhe Xu<sup style="font-size:10px">2</sup></a>
        </span></center>
      </div>

      <div class="col-md-2">
        <center><span style="font-size:16px">
          <a href="https://github.com/xjwxjw">Jingwei Xu<sup style="font-size:10px">3</sup></a>
        </span></center>
      </div>

      <div class="col-md-2">
        <center><span style="font-size:16px">
          <a href="https://www.sifeiliu.net/">Sifei Liu<sup style="font-size:10px">4</sup></a>
        </span></center>
      </div>

      <div class="col-md-2">
        <center><span style="font-size:16px">
          <a href="https://xiaolonw.github.io/">Xiaolong Wang<sup style="font-size:10px">1</sup></a>
        </span></center>
      </div>

      <div class="col-md-1">
      </div>
    </div>

    <!--------------------- AFFILIATIONS --------------------->
    <div class="gap-5"></div>
    <div class="row">
      <div class="col-md-2">
      </div>

      

      <div class="col-md-2">
        <center><span style="font-size:16px">
          UC San Diego<sup style="font-size:10px">1</sup>
        </span></center>
      </div>

      <div class="col-md-2">
        <center><span style="font-size:16px">
          UC Berkeley<sup style="font-size:10px">2</sup>
        </span></center>
      </div>

      <div class="col-md-2">
        <center><span style="font-size:16px">
          Shanghai Jiao Tong University<sup style="font-size:10px">3</sup>
        </span></center>
      </div>
        
      <div class="col-md-2">
        <center><span style="font-size:16px">
          NVIDIA<sup style="font-size:10px">4</sup>
        </span></center>
      </div>

      <div class="col-md-3">
      </div>
    </div>
    <!--------------------- paper and code link --------------------->
    <div class="gap-5"></div>
    <div class="row">
      <div class="col-md-4">
      </div>

      <div class="col-md-2">
        <center><span style="font-size:16px">
          <a href="https://arxiv.org/pdf/2012.05522.pdf">[arXiv]</a>
        </span></center>
      </div>

      <div class="col-md-3">
        <center><span style="font-size:16px">
          <a href="https://github.com/jiashunwang/Long-term-Motion-in-3D-Scenes">[code]</a>
        </span></center>
      </div>

      <div class="col-md-4">
      </div>
    </div>
    <!--------------------- teaser --------------------->
    <div class="gap-5"></div>
    <div class="img" style="text-align:center">
      <img src="./sources/teaser10.png" style="margin:0.2em;max-width:75%">
    </div>

    <!--------------------- abstract --------------------->
    <div class="gap-20"></div>
    <b><span style="font-size:20px">Abstract:</span></b><br>
    <div class="gap-5"></div>

    <p> Synthesizing 3D human motion plays an important role in many graphics applications as well as understanding human activity. While many efforts have been made on generating realistic and natural human motion, most approaches neglect the importance of modeling human-scene interactions and affordances. On the other hand, affordance reasoning (e.g., standing on the floor or sitting on the chair) has mainly been studied with static human pose and gestures, and it has rarely been addressed with human motion. In this paper, we propose to bridge human motion synthesis and scene affordance reasoning. We present a hierarchical generative framework which synthesizes long-term 3D human motion conditioning on the 3D scene structure. We also further enforce multiple geometry constraints between the human mesh and scene point clouds via optimization to improve realistic synthesis. Our experiments show significant improvements over previous approaches on generating natural and physically plausible human motion in a scene.    <hr>
<!--
    <b><span style="font-size:20px">Methods:</span></b><br>

    <div class="gap-20"></div>
    <div class="img" style="text-align:center">
      <img src="./figs/methodv2.png" style="margin:0.2em;max-width:75%">
    </div>
    <div class="gap-20"></div>
    Our model takes N video frames as inputs and predicts the object locations for the future T timesteps, as illustrated above. We first extract the image feature representation using a ConvNet for each frame, and then apply RoI pooling to obtain object-centric visual features. These object feature representations are forwarded to the interaction modules to perform interaction reasoning and predict future object locations.
The whole pipeline is trained end-to-end by minimizing the loss between predicted and the ground-truth object locations. Since the parameters of each interaction module is shared so we can apply this process recurrently over time to an arbitrary T during testing.
--> 
    

    <div class="gap-20"></div>
    <b><span style="font-size:20px">Video:</span></b><br>
    <div class="gap-5"></div>

    <div class="gap-20"></div>
    <div class="embed-responsive embed-responsive-16by9">
    <iframe width="716" height="403" src="https://www.youtube.com/embed/iDrjIGrJz2Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
    </div>
    <hr>

    
    <b><span style="font-size:20px">Methods:</span></b><br>

    <div class="gap-20"></div>
    <div class="img" style="text-align:center">
      <img src="./sources/pipeline.png" style="margin:0.2em;max-width:90%">
    </div>
    <div class="gap-20"></div>
    We first generate the sub-goal bodies on the sub-goal positions. Sub-goal bodies are in gray color. Then we divide it into several short-term start/end pairs and synthesize short-term motion each. Finally we use an optimization process to help to connect all these short-term motion to a long-term motion.
    <hr>
    
  
    <b><span style="font-size:20px">Qualitative Results:</span></b><br>
    <div class="gap-20"></div>
  
    <div class="embed-responsive embed-responsive-32by9">  
    <video width="800px" playsinline autoplay loop preload muted> <source src="./sources/wp1.mp4" type=video/mp4><video>
    </div>
      
    <div class="embed-responsive embed-responsive-32by9">
    <video width="800px" playsinline autoplay loop preload muted> <source src="./sources/wp2.mp4" type=video/mp4><video>
    </div>
    
    <div class="embed-responsive embed-responsive-32by9">
    <video width="800px" playsinline autoplay loop preload muted> <source src="./sources/wp3.mp4" type=video/mp4><video>
    </div>
      
    <div class="embed-responsive embed-responsive-32by9">
    <video width="800px" playsinline autoplay loop preload muted> <source src="./sources/wp4_.mp4" type=video/mp4><video>
    </div>
      
    <div class="embed-responsive embed-responsive-32by9">
    <video width="800px" playsinline autoplay loop preload muted> <source src="./sources/wp5.mp4" type=video/mp4><video>
    </div>
      
    <div class="embed-responsive embed-responsive-32by9">
    <video width="800px" playsinline autoplay loop preload muted> <source src="./sources/wp6.mp4" type=video/mp4><video>
    </div>
    
    <!--
    <div class="gap-10"></div>
    <center><b><span style="font-size:18px"> Changing body shape </span></b></center>
    <div class="gap-10"></div>

    <div class="row no-gutters">
      <div class="col-3">
        <img src="./sources/shape_eg1.gif" style="margin:1em;max-width:95%">
      </div>

      <div class="col-3">
        <img src="./sources/shape_eg2.gif" style="margin:1em;max-width:95%">
      </div>

      <div class="col-3">
        <img src="./sources/shape_eg3.gif" style="margin:1em;max-width:95%">
      </div>
    </div>
      
    <div class="gap-10"></div>
    <center><b><span style="font-size:18px"> Changing latent z </span></b></center>
    <div class="gap-10"></div>
    <center>
    <div class="row no-gutters">
      <div class="col-3">
        <img src="./sources/z_eg1.gif" style="margin:1em;max-width:95%">
      </div>

      <div class="col-3">
        <img src="./sources/z_eg2.gif" style="margin:1em;max-width:95%">
      </div>

      <div class="col-3">
        <img src="./sources/z_eg3.gif" style="margin:1em;max-width:95%">
      </div>
    </div>
    </center>
    <hr>
    -->
<!--
    <b><span style="font-size:20px">Paper:</span></b><br>
    <div class="img" style="text-align:center">
      <a href="https://arxiv.org/pdf/2008.02265.pdf">
        <img src="./figs/thumbnail.png" style="margin:1.2em;max-width:95%">
      </a>
    </div>

    <hr>


    <b><span style="font-size:20px">Bibtex:</span></b>
    <br>
    <div class="gap-10"></div>
    <pre>
    @article{qi2020learning,
     author={Qi, Haozhi and Wang, Xiaolong and Pathak, Deepak and Ma, Yi and Malik, Jitendra},
     title={Learning Long-term Visual Dynamics with Region Proposal Interaction Networks},
     journal={arXiv},
     year={2020}
    }
    </pre>
    <hr>
--> 
    <hr>

    <b><span style="font-size:20px">Paper:</span></b><br>
    <div class="img" style="text-align:center">
      <a href="https://arxiv.org/pdf/2012.05522.pdf">
        <img src="./sources/paper1.png" style="margin:1.2em;max-width:95%">
      </a>
    </div>
      
      
    <hr>
    <b><span style="font-size:20px">Bibtex:</span></b>
    <br>
    <div class="gap-10"></div>
    <pre>
    @misc{wang2020synthesizing,
      title={Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes}, 
      author={Jiashun Wang and Huazhe Xu and Jingwei Xu and Sifei Liu and Xiaolong Wang},
      year={2020},
      eprint={2012.05522},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
    }
    </pre>
      
    
</html>
