{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, glob\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_global_T(x_batch, cam_intrisic, max_depth):\n",
    "    xt_batch = x_batch[:,:3]\n",
    "    xr_batch = x_batch[:,3:]\n",
    "\n",
    "    fx_batch = cam_intrisic[:,0,0]\n",
    "    fy_batch = cam_intrisic[:,1,1]\n",
    "    px_batch = cam_intrisic[:,0,2]\n",
    "    py_batch = cam_intrisic[:,1,2]\n",
    "    s_ = 1.0 / torch.max(px_batch, py_batch)\n",
    "\n",
    "    z = (xt_batch[:, 2]+1.0)/2.0 * max_depth\n",
    "\n",
    "    x = xt_batch[:,0] * z / s_ / fx_batch\n",
    "    y = xt_batch[:,1] * z / s_ / fy_batch\n",
    "    \n",
    "    xt_batch_recoverd = torch.stack([x,y,z],dim=-1)\n",
    "\n",
    "    return torch.cat([xt_batch_recoverd, xr_batch],dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_3D_rot(x_batch):\n",
    "    xt = x_batch[:,:3]\n",
    "    xr = x_batch[:,3:9]\n",
    "    xb = x_batch[:,9:]\n",
    "\n",
    "    xr_mat = ContinousRotReprDecoder.decode(xr) # return [:,3,3]\n",
    "    xr_aa = ContinousRotReprDecoder.matrot2aa(xr_mat) # return [:,3]\n",
    "\n",
    "    return torch.cat([xt, xr_aa, xb], dim=-1)\n",
    "\n",
    "\n",
    "def body_params_encapsulate(x_body_rec, to_numpy=True, batched=False):\n",
    "    \n",
    "    if to_numpy:\n",
    "        x_body_rec_np = x_body_rec.detach().cpu().numpy()\n",
    "    else:\n",
    "        x_body_rec_np = x_body_rec\n",
    "        \n",
    "    \n",
    "    if batched:\n",
    "        body_params_batch_rec={}\n",
    "        body_params_batch_rec['transl'] = x_body_rec_np[:,:3]\n",
    "        body_params_batch_rec['global_orient'] = x_body_rec_np[:,3:6]\n",
    "        body_params_batch_rec['betas'] = x_body_rec_np[:,6:16]\n",
    "        body_params_batch_rec['body_pose'] = x_body_rec_np[:,16:48]\n",
    "        body_params_batch_rec['left_hand_pose'] = x_body_rec_np[:,48:60]\n",
    "        body_params_batch_rec['right_hand_pose'] = x_body_rec_np[:,60:]\n",
    "        \n",
    "        return body_params_batch_rec\n",
    "    \n",
    "    else:\n",
    "        n_batch = x_body_rec_np.shape[0]\n",
    "        rec_list = []\n",
    "\n",
    "        for b in range(n_batch):\n",
    "            body_params_batch_rec={}\n",
    "            body_params_batch_rec['transl'] = x_body_rec_np[b:b+1,:3]\n",
    "            body_params_batch_rec['global_orient'] = x_body_rec_np[b:b+1,3:6]\n",
    "            body_params_batch_rec['betas'] = x_body_rec_np[b:b+1,6:16]\n",
    "            body_params_batch_rec['body_pose'] = x_body_rec_np[b:b+1,16:48]\n",
    "            #body_params_batch_rec['left_hand_pose'] = x_body_rec_np[b:b+1,48:60]\n",
    "            #body_params_batch_rec['right_hand_pose'] = x_body_rec_np[b:b+1,60:]\n",
    "            rec_list.append(body_params_batch_rec)\n",
    "\n",
    "        return rec_list\n",
    "\n",
    "\n",
    "def data_preprocessing(img, modality, target_domain_size=[128, 128]):\n",
    "\n",
    "    \"\"\"\n",
    "    input:\n",
    "        - img (depthmap or semantic map): [height, width].\n",
    "        - modality: 'depth' or 'seg'\n",
    "    output:\n",
    "        canvas: with shape of target_domain_size, where the input is in the\n",
    "                center tightly, with shape target_domain_size\n",
    "        factor: the resizing factor\n",
    "    \"\"\"\n",
    "\n",
    "    # prepare the canvas\n",
    "    img_shape_o = img.shape\n",
    "    canvas = torch.zeros([1,1]+target_domain_size, dtype=torch.float32,\n",
    "                         device=torch.device(device))\n",
    "\n",
    "\n",
    "    # filter out unavailable values\n",
    "    if modality == 'depth':\n",
    "        img[img>6.0]=6.0\n",
    "\n",
    "    if modality == 'seg':\n",
    "        img[img>41] = 41\n",
    "\n",
    "\n",
    "\n",
    "    ## rescale to [-1,1]\n",
    "    max_val = torch.max(img)\n",
    "    _img = 2* img / max_val - 1.0\n",
    "\n",
    "    ## put _img to the canvas\n",
    "    if img_shape_o[0]>= img_shape_o[1]:\n",
    "        factor = float(target_domain_size[0]) / img_shape_o[0]\n",
    "        target_height = target_domain_size[0]\n",
    "        target_width = int(img_shape_o[1] * factor) //2 *2 \n",
    "\n",
    "        # for depth map we use bilinear interpolation in resizing\n",
    "        # for segmentation map we use bilinear interpolation as well.\n",
    "        # note that float semantic label is not real in practice, but\n",
    "        # helpful in our work\n",
    "        target_size = [target_height, target_width]\n",
    "\n",
    "        _img = _img.view(1,1,img_shape_o[0],img_shape_o[1])\n",
    "        img_resize = F.interpolate(_img, size=target_size, mode='bilinear',\n",
    "                                    align_corners=False)\n",
    "\n",
    "        na = target_width\n",
    "        nb = target_domain_size[1]\n",
    "        lower = (nb //2) - (na //2)\n",
    "        upper = (nb //2) + (na //2)\n",
    "\n",
    "        canvas[:,:,:, lower:upper] = img_resize\n",
    "\n",
    "\n",
    "    else:\n",
    "        factor = float(target_domain_size[1]) / img_shape_o[1]\n",
    "\n",
    "        target_height = int(factor*img_shape_o[0]) //2 *2\n",
    "        target_width = target_domain_size[1]\n",
    "\n",
    "        target_size = [target_height, target_width]\n",
    "        _img = _img.view(1,1,img_shape_o[0],img_shape_o[1])\n",
    "        img_resize = F.interpolate(_img, size=target_size, mode='bilinear',\n",
    "                                    align_corners=False)\n",
    "\n",
    "        na = target_height\n",
    "        nb = target_domain_size[0]\n",
    "        lower = (nb //2) - (na //2)\n",
    "        upper = (nb //2) + (na //2)\n",
    "\n",
    "        canvas[:,:,lower:upper, :] = img_resize\n",
    "\n",
    "    return canvas, factor, max_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scipy_matfile_parse(filename):\n",
    "    '''\n",
    "    parse data from files and put them to GPU\n",
    "    Note that this function is for demo, and is different from the ones used in other places.\n",
    "    '''\n",
    "    data = sio.loadmat(filename)\n",
    "    depth0_np = data['depth']\n",
    "    seg0_np = data['seg']\n",
    "\n",
    "    ## change them to torch tensor\n",
    "    depth0 = torch.tensor(depth0_np, dtype=torch.float32, device=torch.device(device))\n",
    "    seg0 = torch.tensor(seg0_np, dtype=torch.float32, device=torch.device(device))\n",
    "\n",
    "    ## pre_processing\n",
    "    depth, factor_d,max_d = data_preprocessing(depth0, 'depth', target_domain_size=[128, 128])\n",
    "    seg, factor_s,_ = data_preprocessing(seg0, 'seg', target_domain_size=[128, 128])\n",
    "\n",
    "\n",
    "    cam_intrinsic_np = data['cam'][0][0]['intrinsic']\n",
    "    cam_intrinsic = torch.tensor(cam_intrinsic_np, dtype=torch.float32, device=torch.device(device)).unsqueeze(0)\n",
    "    cam_extrinsic_np = data['cam'][0][0]['extrinsic']\n",
    "    cam_extrinsic_np = np.linalg.inv(cam_extrinsic_np)\n",
    "    cam_extrinsic = torch.tensor(cam_extrinsic_np, dtype=torch.float32, device=torch.device(device)).unsqueeze(0)\n",
    "\n",
    "    return depth, seg, max_d.view(1), cam_intrinsic, cam_extrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data,seq):\n",
    "    dataset=[]\n",
    "    for i in range(len(data)):\n",
    "        body_t = data[i]['transl']\n",
    "        body_r = data[i]['global_orient']\n",
    "        body_shape=data[i]['betas']\n",
    "        body_pose = data[i]['pose_embedding']\n",
    "        body_lhp = data[i]['left_hand_pose']\n",
    "        body_rhp = data[i]['right_hand_pose']\n",
    "        body = np.concatenate([body_t, body_r, body_shape, \n",
    "                               body_pose, body_lhp, body_rhp\n",
    "                               ],\n",
    "                               axis=-1)\n",
    "       \n",
    "        dataset.append(body)\n",
    "   \n",
    "    input_list=[]\n",
    "    gt_list=[]\n",
    "    seq=seq\n",
    "    for i in range(len(dataset)):\n",
    "        \n",
    "        try:\n",
    "            input_=torch.tensor(np.concatenate([dataset[i],dataset[i+seq*61]]), dtype=torch.float32)\n",
    "            t_list=[]\n",
    "            for temp_i in range(60):\n",
    "                t_list.append(dataset[i+seq*temp_i+1])  \n",
    "            gt=torch.tensor(np.concatenate(t_list),dtype=torch.float32)\n",
    "            \n",
    "\n",
    "            if torch.max(np.isnan(input_))==0 and torch.max(np.isnan(gt))==0:\n",
    "                input_list.append(input_)\n",
    "                gt_list.append(gt)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return input_list,gt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training BasementSittingBooth_00142_01 1395\n",
      "training BasementSittingBooth_00145_01 1590\n",
      "training BasementSittingBooth_03452_01 1366\n",
      "training MPH112_00034_01 1775\n",
      "training MPH112_00150_01 1503\n",
      "training MPH112_00151_01 746\n",
      "training MPH112_00157_01 1179\n",
      "training MPH112_00169_01 1184\n",
      "training MPH112_03515_01 1018\n",
      "training MPH11_00034_01 2112\n",
      "training MPH11_00150_01 2125\n",
      "training MPH11_00151_01 1396\n",
      "training MPH11_03515_01 1852\n",
      "testing MPH16_00157_01 1723\n",
      "testing MPH16_03301_01 899\n",
      "testing MPH1Library_00034_01 2149\n",
      "training MPH8_00168_01 2669\n",
      "training MPH8_03301_01 1587\n",
      "testing N0SittingBooth_00162_01 1117\n",
      "testing N0SittingBooth_00169_01 900\n",
      "testing N0SittingBooth_00169_02 768\n",
      "testing N0SittingBooth_03301_01 1026\n",
      "testing N0SittingBooth_03403_01 1167\n",
      "training N0Sofa_00034_01 2583\n",
      "training N0Sofa_00034_02 1384\n",
      "training N0Sofa_00141_01 2204\n",
      "training N0Sofa_00145_01 1980\n",
      "training N3Library_00157_01 905\n",
      "training N3Library_00157_02 652\n",
      "training N3Library_03301_01 765\n",
      "training N3Library_03301_02 590\n",
      "training N3Library_03375_01 1038\n",
      "training N3Library_03375_02 402\n",
      "training N3Library_03403_01 608\n",
      "training N3Library_03403_02 922\n",
      "training N3Office_00034_01 2090\n",
      "training N3Office_00139_01 1275\n",
      "training N3Office_00139_02 1701\n",
      "training N3Office_00150_01 2442\n",
      "training N3Office_00153_01 2921\n",
      "training N3Office_00159_01 1975\n",
      "training N3Office_03301_01 1982\n",
      "testing N3OpenArea_00157_01 934\n",
      "testing N3OpenArea_00157_02 1263\n",
      "testing N3OpenArea_00158_01 824\n",
      "testing N3OpenArea_00158_02 1287\n",
      "testing N3OpenArea_03301_01 994\n",
      "testing N3OpenArea_03403_01 603\n",
      "training Werkraum_03301_01 690\n",
      "training Werkraum_03403_01 673\n",
      "training Werkraum_03516_01 1397\n",
      "training Werkraum_03516_02 1159\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2020)\n",
    "s1_data_training=[]\n",
    "s1_data_testing=[]\n",
    "testing_scene= ['MPH16','MPH1Library', 'N0SittingBooth','N3OpenArea']\n",
    "sdf_path = scene_sdf_path = './data/prox/sdf' \n",
    "total_file_list=sorted(os.listdir('./data/prox/PROXD/'))\n",
    "\n",
    "for each in total_file_list:\n",
    "    scene_name=each[:-9]\n",
    "    with open(os.path.join(scene_sdf_path, scene_name+'.json')) as f:\n",
    "        sdf_data = json.load(f)\n",
    "        grid_min = np.array(sdf_data['min'])\n",
    "        grid_max = np.array(sdf_data['max'])\n",
    "        grid_dim = sdf_data['dim']\n",
    "    sdf = np.load(os.path.join(scene_sdf_path, scene_name + '_sdf.npy')).reshape(grid_dim, grid_dim, grid_dim)\n",
    "    \n",
    "    scene_name=each[:-9]\n",
    "    \n",
    "    filelist=os.listdir('./data/prox/PROXD/'+each+'/results')\n",
    "    filelist=sorted(filelist)\n",
    "    temp_data_file=filelist\n",
    "    data=[]\n",
    "    for i in range(len(temp_data_file)):\n",
    "        f=open('./data/prox/PROXD/'+each+'/results/'+temp_data_file[i]+'/000.pkl','rb')\n",
    "        temp=pickle.load(f)\n",
    "        data.append(temp)\n",
    "    sample_data=[]\n",
    "\n",
    "    for i in range(0,len(data),1):\n",
    "        sample_data.append(data[i])\n",
    "        \n",
    "    input_list,middle_list=create_dataset(sample_data,1)\n",
    "    \n",
    "    scene_mesh = o3d.io.read_triangle_mesh('./data/Proxe/scenes_downsampled/'+scene_name+'.ply')\n",
    "    \n",
    "    cam_ext_path='./data/prox/cam2world/'+scene_name+'.json'\n",
    "    f=open(cam_ext_path,'r')\n",
    "    contents = f.read();\n",
    "    cam_ext = json.loads(contents)\n",
    "    cam_= np.linalg.inv(cam_ext)\n",
    "    \n",
    "    \n",
    "    scene_verts = torch.tensor(np.asarray(scene_mesh.transform(cam_).vertices),dtype=torch.float32)\n",
    "    \n",
    "    grid_min = torch.tensor(np.array(sdf_data['min']),dtype=torch.float32)\n",
    "    grid_max = torch.tensor(np.array(sdf_data['max']),dtype=torch.float32)\n",
    "    if scene_name in testing_scene:\n",
    "        for i in range(len(input_list)):\n",
    "            s1_data_testing.append([input_list[i],middle_list[i],each,scene_name,sdf,scene_verts,np.array(cam_ext),grid_min,grid_max])\n",
    "        print('testing',each,i)\n",
    "    else:\n",
    "        for i in range(len(input_list)):\n",
    "            s1_data_training.append([input_list[i],middle_list[i],each,scene_name,sdf,scene_verts,np.array(cam_ext),grid_min,grid_max])\n",
    "        print('training',each,i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_data_=[]\n",
    "s1_data=s1_data_training\n",
    "for i in range(len(s1_data)):\n",
    "    if torch.sqrt(((s1_data[i][0][0][:3]-s1_data[i][0][1][:3]) ** 2).sum(dim=-1)).mean()>=0.5:\n",
    "        s1_data_.append(s1_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/routepose_training_data.npy',np.array(s1_data_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_data_=[]\n",
    "s1_data=s1_data_testing\n",
    "for i in range(len(s1_data)):\n",
    "    if torch.sqrt(((s1_data[i][0][0][:3]-s1_data[i][0][1][:3]) ** 2).sum(dim=-1)).mean()>=0.5:\n",
    "        s1_data_.append(s1_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/routepose_testing_data.npy',np.array(s1_data_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
